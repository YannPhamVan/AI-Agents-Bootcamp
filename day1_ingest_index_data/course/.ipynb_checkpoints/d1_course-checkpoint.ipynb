{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387e668f-ca6a-4da1-b074-4ce199d454d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81c6f7-008c-478a-a9ab-81e446f9f73b",
   "metadata": {},
   "source": [
    "## Understanding Frontmatter\n",
    "We will also need a library for parsing frontmatter - a popular documentation format commonly used for modern frameworks like Jekyll, Hugo, and Next.js.\n",
    "\n",
    "It looks like this:\n",
    "```\n",
    "---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "date: \"2024-01-15\"\n",
    "tags: [\"ai\", \"machine-learning\", \"tutorial\"]\n",
    "difficulty: \"beginner\"\n",
    "---\n",
    "\n",
    "# Getting Started with AI\n",
    "\n",
    "This is the main content of the document written in **Markdown**.\n",
    "\n",
    "You can include code blocks, links, and other formatting here.\n",
    "\n",
    "```\n",
    "This is the main content of the document written in **Markdown**.\n",
    "\n",
    "You can include code blocks, links, and other formatting here.\n",
    "\n",
    "This format is called \"frontmatter\". The section between the --- markers contains YAML metadata that describes the document, while everything below is regular Markdown content. This is very useful because we can extract structured information (like title, tags, difficulty level) along with the content.\n",
    "\n",
    "This is how we read it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247ecd3d-45ce-460e-bf2f-7ea4abdc257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.md', 'r', encoding='utf-8') as f:\n",
    "    post = frontmatter.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a70d2a-9186-4601-a897-004ee46712de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Started with AI\n",
      "['ai', 'machine-learning', 'tutorial']\n"
     ]
    }
   ],
   "source": [
    "# Access metadata\n",
    "print(post.metadata['title'])  # \"Getting Started with AI\"\n",
    "print(post.metadata['tags'])   # [\"ai\", \"machine-learning\", \"tutorial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c758089-6631-4e88-af0f-d06c0a82d288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Getting Started with AI\n",
      "\n",
      "This is the main content of the document written in **Markdown**.\n",
      "\n",
      "You can include code blocks, links, and other formatting here.\n"
     ]
    }
   ],
   "source": [
    "# Access content\n",
    "print(post.content)  # The markdown content without frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05d052-a96d-4247-a4ef-a8d1d23af0ae",
   "metadata": {},
   "source": [
    "We can also get all the metadata and content at the same time using the post.to_dict() method.\n",
    "## Sample Repositories\n",
    "Now that we know how to process a single markdown file, let's find a repo with multiple files that we will use as our knowledge base.\n",
    "\n",
    "We will work with multiple repositories:\n",
    "- https://github.com/DataTalksClub/faq (source for https://datatalks.club/faq/) - FAQ for DataTalks.Club courses\n",
    "- https://github.com/evidentlyai/docs/ - docs for Evidently AI library\n",
    "There are multiple ways you can download a GitHub repo.\n",
    "First, you can clone it using git, then we process each file and prepare it for ingestion into our search system.\n",
    "Alternatively, we can download the entire repository as a zip file and process all the content.\n",
    "## Working with Zip Archives\n",
    "The second option is easier and more efficient for our use case.\n",
    "We don't even need to save the zip archive - we can load it into our Python process memory and extract all the data we need from there.\n",
    "So the plan:\n",
    "- Use requests for downloading the zip archive from GitHub\n",
    "- Open the archive using built-in zipfile and io modules\n",
    "- Iterate over all .md and .mdx files in the repo\n",
    "- Collect the results into a list\n",
    "\n",
    "Let's implement it step by step.\n",
    "\n",
    "Next, we download the repository as a zip file. GitHub provides a convenient URL format for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8e78b4-1974-422d-8e8a-32bd8d6b4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104ddfd-7a58-4bd3-a132-fea6f0de3665",
   "metadata": {},
   "source": [
    "Next, we download the repository as a zip file. GitHub provides a convenient URL format for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9b065b-11ea-4745-82cf-dd853040fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985a228-31cd-4719-a6ee-dac5f71ce725",
   "metadata": {},
   "source": [
    "Let's look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202f85cf-95a5-4089-9a06-ac79594d30d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '# DataTalks.Club FAQ\\n\\nA static site generator for DataTalks.Club course FAQs with automated AI-powered FAQ maintenance.\\n\\n## Features\\n\\n- **Static Site Generation**: Converts markdown FAQs to a beautiful, searchable HTML site\\n- **Automated FAQ Management**: AI-powered bot that processes new FAQ proposals\\n- **Intelligent Triage**: Automatically determines if proposals should create new entries, update existing ones, or are duplicates\\n- **GitHub Integration**: Seamless workflow via GitHub Issues and Pull Requests\\n\\n## Project Structure\\n\\n```\\nfaq/\\n├── _questions/              # FAQ content organized by course\\n│   ├── machine-learning-zoomcamp/\\n│   │   ├── _metadata.yaml   # Course configuration\\n│   │   ├── general/         # General course questions\\n│   │   ├── module-1/        # Module-specific questions\\n│   │   └── ...\\n│   ├── data-engineering-zoomcamp/\\n│   └── ...\\n├── _layouts/                # Jinja2 HTML templates\\n│   ├── base.html\\n│   ├── course.html\\n│   └── index.html\\n├── assets/                  # CSS and static assets\\n├── faq_automation/          # FAQ automation module\\n│   ├── core.py             # Core FAQ processing functions\\n│   ├── rag_agent.py        # AI-powered decision agent\\n│   ├── actions.py          # GitHub Actions integration\\n│   └── cli.py              # Command-line interface\\n├── tests/                   # Test suite\\n├── generate_website.py      # Main site generator\\n└── Makefile                # Build commands\\n```\\n\\n## Contributing FAQ Entries\\n\\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for detailed instructions.\\n\\n\\n## Development\\n\\n### Setup\\n\\n```bash\\n# Install dependencies\\nuv sync --dev\\n```\\n\\nFor testing the FAQ automation locally, you\\'ll need to set your OpenAI API key:\\n\\n```bash\\nexport OPENAI_API_KEY=\\'your-api-key-here\\'\\n```\\n\\nOr add it to your shell configuration file (e.g., `~/.bashrc`, `~/.zshrc`).\\n\\n### Running Locally\\n\\nTo test the FAQ automation locally, create a `test_issue.txt` file:\\n\\n```bash\\ncat > test_issue.txt << \\'EOF\\'\\n### Course\\nmachine-learning-zoomcamp\\n\\n### Question\\nHow do I check my Python version?\\n\\n### Answer\\nRun `python --version` in your terminal.\\nEOF\\n```\\n\\nThen process the FAQ proposal:\\n\\n```bash\\nuv run python -m faq_automation.cli \\\\\\n  --issue-body \"$(cat test_issue.txt)\" \\\\\\n  --issue-number 42\\n```\\n\\n### Testing\\n\\n```bash\\n# Generate static website\\nmake website\\n\\n# Run all tests\\nmake test\\n\\n# Run unit tests only\\nmake test-unit\\n\\n# Run integration tests only\\nmake test-int\\n```\\n\\nSee [testing documentation](tests/README.md) for detailed information about the test suite, including how to run specific test files or methods, test coverage details, and guidelines for adding new tests.\\n\\n## Architecture\\n\\n### Site Generation Pipeline\\n\\n1. **Collection** (`collect_questions()`):\\n   - Reads all markdown files from `_questions/`\\n   - Parses YAML frontmatter\\n   - Loads course metadata for section ordering\\n\\n2. **Processing** (`process_markdown()`):\\n   - Converts markdown to HTML\\n   - Applies syntax highlighting (Pygments)\\n   - Auto-links plain text URLs\\n   - Handles image placeholders\\n\\n3. **Sorting** (`sort_sections_and_questions()`):\\n   - Orders sections per `_metadata.yaml`\\n   - Sorts questions by `sort_order` field\\n\\n4. **Rendering** (`generate_site()`):\\n   - Applies Jinja2 templates\\n   - Generates course pages and index\\n   - Copies assets to `_site/`', 'filename': 'faq-main/readme.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61037193-fcdc-464a-9ff2-c5ab976128e8",
   "metadata": {},
   "source": [
    "For processing Evidently docs we also need .mdx files (React markdown), so we can modify the code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40787433-4d1a-4004-a6a3-ca5f6a89ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/evidentlyai/docs/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "424526df-94dc-4b13-b8af-7130201c4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    if not (filename.endswith('.md') or filename.endswith('.mdx')):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49eec1f1-018f-4459-a504-eb4d28f45695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Delete Plant', 'openapi': 'DELETE /plants/{id}', 'content': '', 'filename': 'docs-main/api-reference/endpoint/delete.mdx'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b89d8-dc5a-42da-b72a-14f1caeaf8a1",
   "metadata": {},
   "source": [
    "## Complete Implementation\n",
    "Let's now put everything together into a reusable function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c8e11b4-4277-4548-b330-ec10f9e77b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a88579-1abb-4e94-86e0-a2539a6f2f79",
   "metadata": {},
   "source": [
    "We can now use this function for different repositories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a0dc95-f67b-4dfb-84e2-ba3a6ac242ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1228\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafea7a5-3aa2-417f-9219-4cafc958ea48",
   "metadata": {},
   "source": [
    "## Data Processing Considerations\n",
    "For FAQ, the data is ready to use. These are small records that we can index (put into a search engine) as is.\n",
    "\n",
    "For Evidently docs, the documents are very large. We need extra processing called \"chunking\" - breaking large documents into smaller, manageable pieces. This is important because:\n",
    "1. Search relevance: Smaller chunks are more specific and relevant to user queries\n",
    "2. Performance: AI models work better with shorter text segments\n",
    "3. Memory limits: Large documents might exceed token limits of language models\n",
    "We will cover chunking techniques in tomorrow's lesson.\n",
    "\n",
    "If you have any suggestions about the course content or want to improve something, let me know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
