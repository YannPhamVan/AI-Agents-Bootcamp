{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387e668f-ca6a-4da1-b074-4ce199d454d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c95150-5c73-4b2f-84c8-f522ab0a10e2",
   "metadata": {},
   "source": [
    "# Day 1: Ingest and Index Your Data\n",
    "Welcome to our crash course!\n",
    "\n",
    "In this course, you'll learn how to build intelligent systems that can understand and interact with your data.\n",
    "\n",
    "We'll create a conversational agent that can answer questions about any GitHub repository - think of it as your personal AI assistant for documentation and code. If you know DeepWiki, it's something similar, but tailored to your GitHub repo.\n",
    "\n",
    "For that, we need to:\n",
    "- Download and process data from the repo\n",
    "- Put it inside a search engine\n",
    "- Make the search engine available to our agent\n",
    "\n",
    "In the first half of the course, we will focus on data preparation.\n",
    "\n",
    "Today, we will do the first part: downloading the data.\n",
    "## GitHub Repo Data\n",
    "On the first day, we will learn how to download and process data from any GitHub repository. We will download the data as a zip archive, process all the text data from there, and make it available for ingesting it later into a search engine.\n",
    "\n",
    "Think of this as preparing a meal - we need to gather and prep all our ingredients (the data) before we can cook (build our AI agent).\n",
    "\n",
    "Today, we will deal with simple cases, when documents are not large.\n",
    "\n",
    "Tomorrow we will deal with more complex cases when documents are big and we also have code.\n",
    "## Environment Setup\n",
    "First, let's prepare the environment. We need Python 3.10 or higher.\n",
    "\n",
    "We will use uv as the package manager. If you don't have uv, let's install it:\n",
    "```\n",
    "pip install uv\n",
    "```\n",
    "Next, create a folder aihero with two subfolders:\n",
    "- course - here you will reproduce all the examples from this email course\n",
    "- project - here you will create your own project\n",
    "Now go to course and run:\n",
    "```\n",
    "uv init\n",
    "uv add requests python-frontmatter\n",
    "uv add --dev jupyter\n",
    "```\n",
    "This will initialize an empty-  Python project with uv and install multiple libraries:\n",
    "- requests for downloading data from GitHub\n",
    "- python-frontmatter for parsing structured metadata in markdown files\n",
    "- jupyter (in dev mode)\n",
    "- \n",
    "The reason we need jupyter in dev mode is because it's only used for development and experimentation, not in the final production code.\n",
    "\n",
    "Let's start Jupyter:\n",
    "```\n",
    "uv run jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81c6f7-008c-478a-a9ab-81e446f9f73b",
   "metadata": {},
   "source": [
    "## Understanding Frontmatter\n",
    "We will also need a library for parsing frontmatter - a popular documentation format commonly used for modern frameworks like Jekyll, Hugo, and Next.js.\n",
    "\n",
    "It looks like this:\n",
    "```\n",
    "---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "date: \"2024-01-15\"\n",
    "tags: [\"ai\", \"machine-learning\", \"tutorial\"]\n",
    "difficulty: \"beginner\"\n",
    "---\n",
    "\n",
    "# Getting Started with AI\n",
    "\n",
    "This is the main content of the document written in **Markdown**.\n",
    "\n",
    "You can include code blocks, links, and other formatting here.\n",
    "\n",
    "```\n",
    "This is the main content of the document written in **Markdown**.\n",
    "\n",
    "You can include code blocks, links, and other formatting here.\n",
    "\n",
    "This format is called \"frontmatter\". The section between the --- markers contains YAML metadata that describes the document, while everything below is regular Markdown content. This is very useful because we can extract structured information (like title, tags, difficulty level) along with the content.\n",
    "\n",
    "This is how we read it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247ecd3d-45ce-460e-bf2f-7ea4abdc257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.md', 'r', encoding='utf-8') as f:\n",
    "    post = frontmatter.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a70d2a-9186-4601-a897-004ee46712de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Started with AI\n",
      "['ai', 'machine-learning', 'tutorial']\n"
     ]
    }
   ],
   "source": [
    "# Access metadata\n",
    "print(post.metadata['title'])  # \"Getting Started with AI\"\n",
    "print(post.metadata['tags'])   # [\"ai\", \"machine-learning\", \"tutorial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c758089-6631-4e88-af0f-d06c0a82d288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Getting Started with AI\n",
      "\n",
      "This is the main content of the document written in **Markdown**.\n",
      "\n",
      "You can include code blocks, links, and other formatting here.\n"
     ]
    }
   ],
   "source": [
    "# Access content\n",
    "print(post.content)  # The markdown content without frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05d052-a96d-4247-a4ef-a8d1d23af0ae",
   "metadata": {},
   "source": [
    "We can also get all the metadata and content at the same time using the post.to_dict() method.\n",
    "## Sample Repositories\n",
    "Now that we know how to process a single markdown file, let's find a repo with multiple files that we will use as our knowledge base.\n",
    "\n",
    "We will work with multiple repositories:\n",
    "- https://github.com/DataTalksClub/faq (source for https://datatalks.club/faq/) - FAQ for DataTalks.Club courses\n",
    "- https://github.com/evidentlyai/docs/ - docs for Evidently AI library\n",
    "\n",
    "There are multiple ways you can download a GitHub repo.\n",
    "First, you can clone it using git, then we process each file and prepare it for ingestion into our search system.\n",
    "\n",
    "Alternatively, we can download the entire repository as a zip file and process all the content.\n",
    "## Working with Zip Archives\n",
    "The second option is easier and more efficient for our use case.\n",
    "We don't even need to save the zip archive - we can load it into our Python process memory and extract all the data we need from there.\n",
    "So the plan:\n",
    "- Use requests for downloading the zip archive from GitHub\n",
    "- Open the archive using built-in zipfile and io modules\n",
    "- Iterate over all .md and .mdx files in the repo\n",
    "- Collect the results into a list\n",
    "\n",
    "Let's implement it step by step.\n",
    "\n",
    "Next, we download the repository as a zip file. GitHub provides a convenient URL format for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8e78b4-1974-422d-8e8a-32bd8d6b4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104ddfd-7a58-4bd3-a132-fea6f0de3665",
   "metadata": {},
   "source": [
    "Next, we download the repository as a zip file. GitHub provides a convenient URL format for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9b065b-11ea-4745-82cf-dd853040fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985a228-31cd-4719-a6ee-dac5f71ce725",
   "metadata": {},
   "source": [
    "Let's look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202f85cf-95a5-4089-9a06-ac79594d30d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '# DataTalks.Club FAQ\\n\\nA static site generator for DataTalks.Club course FAQs with automated AI-powered FAQ maintenance.\\n\\n## Features\\n\\n- **Static Site Generation**: Converts markdown FAQs to a beautiful, searchable HTML site\\n- **Automated FAQ Management**: AI-powered bot that processes new FAQ proposals\\n- **Intelligent Triage**: Automatically determines if proposals should create new entries, update existing ones, or are duplicates\\n- **GitHub Integration**: Seamless workflow via GitHub Issues and Pull Requests\\n\\n## Project Structure\\n\\n```\\nfaq/\\n├── _questions/              # FAQ content organized by course\\n│   ├── machine-learning-zoomcamp/\\n│   │   ├── _metadata.yaml   # Course configuration\\n│   │   ├── general/         # General course questions\\n│   │   ├── module-1/        # Module-specific questions\\n│   │   └── ...\\n│   ├── data-engineering-zoomcamp/\\n│   └── ...\\n├── _layouts/                # Jinja2 HTML templates\\n│   ├── base.html\\n│   ├── course.html\\n│   └── index.html\\n├── assets/                  # CSS and static assets\\n├── faq_automation/          # FAQ automation module\\n│   ├── core.py             # Core FAQ processing functions\\n│   ├── rag_agent.py        # AI-powered decision agent\\n│   ├── actions.py          # GitHub Actions integration\\n│   └── cli.py              # Command-line interface\\n├── tests/                   # Test suite\\n├── generate_website.py      # Main site generator\\n└── Makefile                # Build commands\\n```\\n\\n## Contributing FAQ Entries\\n\\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for detailed instructions.\\n\\n\\n## Development\\n\\n### Setup\\n\\n```bash\\n# Install dependencies\\nuv sync --dev\\n```\\n\\nFor testing the FAQ automation locally, you\\'ll need to set your OpenAI API key:\\n\\n```bash\\nexport OPENAI_API_KEY=\\'your-api-key-here\\'\\n```\\n\\nOr add it to your shell configuration file (e.g., `~/.bashrc`, `~/.zshrc`).\\n\\n### Running Locally\\n\\nTo test the FAQ automation locally, create a `test_issue.txt` file:\\n\\n```bash\\ncat > test_issue.txt << \\'EOF\\'\\n### Course\\nmachine-learning-zoomcamp\\n\\n### Question\\nHow do I check my Python version?\\n\\n### Answer\\nRun `python --version` in your terminal.\\nEOF\\n```\\n\\nThen process the FAQ proposal:\\n\\n```bash\\nuv run python -m faq_automation.cli \\\\\\n  --issue-body \"$(cat test_issue.txt)\" \\\\\\n  --issue-number 42\\n```\\n\\n### Testing\\n\\n```bash\\n# Generate static website\\nmake website\\n\\n# Run all tests\\nmake test\\n\\n# Run unit tests only\\nmake test-unit\\n\\n# Run integration tests only\\nmake test-int\\n```\\n\\nSee [testing documentation](tests/README.md) for detailed information about the test suite, including how to run specific test files or methods, test coverage details, and guidelines for adding new tests.\\n\\n## Architecture\\n\\n### Site Generation Pipeline\\n\\n1. **Collection** (`collect_questions()`):\\n   - Reads all markdown files from `_questions/`\\n   - Parses YAML frontmatter\\n   - Loads course metadata for section ordering\\n\\n2. **Processing** (`process_markdown()`):\\n   - Converts markdown to HTML\\n   - Applies syntax highlighting (Pygments)\\n   - Auto-links plain text URLs\\n   - Handles image placeholders\\n\\n3. **Sorting** (`sort_sections_and_questions()`):\\n   - Orders sections per `_metadata.yaml`\\n   - Sorts questions by `sort_order` field\\n\\n4. **Rendering** (`generate_site()`):\\n   - Applies Jinja2 templates\\n   - Generates course pages and index\\n   - Copies assets to `_site/`', 'filename': 'faq-main/readme.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61037193-fcdc-464a-9ff2-c5ab976128e8",
   "metadata": {},
   "source": [
    "For processing Evidently docs we also need .mdx files (React markdown), so we can modify the code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40787433-4d1a-4004-a6a3-ca5f6a89ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/evidentlyai/docs/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "424526df-94dc-4b13-b8af-7130201c4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    if not (filename.endswith('.md') or filename.endswith('.mdx')):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49eec1f1-018f-4459-a504-eb4d28f45695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Delete Plant', 'openapi': 'DELETE /plants/{id}', 'content': '', 'filename': 'docs-main/api-reference/endpoint/delete.mdx'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b89d8-dc5a-42da-b72a-14f1caeaf8a1",
   "metadata": {},
   "source": [
    "## Complete Implementation\n",
    "Let's now put everything together into a reusable function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c8e11b4-4277-4548-b330-ec10f9e77b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a88579-1abb-4e94-86e0-a2539a6f2f79",
   "metadata": {},
   "source": [
    "We can now use this function for different repositories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a0dc95-f67b-4dfb-84e2-ba3a6ac242ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1228\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafea7a5-3aa2-417f-9219-4cafc958ea48",
   "metadata": {},
   "source": [
    "## Data Processing Considerations\n",
    "For FAQ, the data is ready to use. These are small records that we can index (put into a search engine) as is.\n",
    "\n",
    "For Evidently docs, the documents are very large. We need extra processing called \"chunking\" - breaking large documents into smaller, manageable pieces. This is important because:\n",
    "1. Search relevance: Smaller chunks are more specific and relevant to user queries\n",
    "2. Performance: AI models work better with shorter text segments\n",
    "3. Memory limits: Large documents might exceed token limits of language models\n",
    "\n",
    "We will cover chunking techniques in tomorrow's lesson.\n",
    "\n",
    "If you have any suggestions about the course content or want to improve something, let me know!\n",
    "## Homework\n",
    "- Create a new uv project in the project directory\n",
    "- Select a GitHub repo with documentation (preferably with .md files)\n",
    "- Download the data from there using the techniques we've learned\n",
    "- Make a post on social media about what you're building\n",
    "# Day 2: Chunking and Intelligent Processing for Data\n",
    "Welcome to Day 2 of our 7-Day AI Agents Email Crash-Course.\n",
    "\n",
    "In the first part of the course, we focus on data preparation – the process of properly preparing data before it can be used for AI agents.\n",
    "\n",
    "**Small and Large Documents**\n",
    "\n",
    "Yesterday (Day 1), we downloaded the data from a GitHub repository and processed it. For some use cases, like the FAQ database, this is sufficient. The questions and answers are small enough. We can put them directly into the search engine.\n",
    "\n",
    "But it's different for the Evidently documentation. These documents are quite large. Let's take a look at this one: https://github.com/evidentlyai/docs/blob/main/docs/library/descriptors.mdx.\n",
    "\n",
    "We could use it as is, but we risk overwhelming our LLMs.\n",
    "\n",
    "**Why We Need to Prepare Large Documents Before Using Them**\n",
    "\n",
    "Large documents create several problems:\n",
    "- Token limits: Most LLMs have maximum input token limits\n",
    "- Cost: Longer prompts cost more money\n",
    "- Performance: LLMs perform worse with very long contexts\n",
    "- Relevance: Not all parts of a long document are relevant to a specific question\n",
    "\n",
    "So we need to split documents into smaller subdocuments. For AI applications like RAG (which we will discuss tomorrow), this process is referred to as \"chunking.\"\n",
    "## Today’s Tasks (Day 2)\n",
    "Today, we will cover multiple ways of chunking data:\n",
    "- Simple character-based chunking\n",
    "- Paragraph and section-based chunking\n",
    "- Intelligent chunking with LLM\n",
    "\n",
    "Just so you know, for the last section, you will need an OpenAI account or an account from an alternative LLM provider such as Groq.\n",
    "## 1. Simple Chunking\n",
    "Let's start with simple chunking. This will be sufficient for most cases.\n",
    "\n",
    "We can continue with the notebook from Day 1. We already downloaded the data from Evidently docs. We put them into the evidently_docs list.\n",
    "\n",
    "This is how the document at index 45 looks like:\n",
    "```\n",
    "{'title': 'LLM regression testing',\n",
    " 'description': 'How to run regression testing for LLM outputs.',\n",
    " 'content': 'In this tutorial, you will learn...'\n",
    "}\n",
    "```\n",
    "The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. For example, for size of 2000 characters, we will have:\n",
    "- chunk 1: 0..2000\n",
    "- Chunk 2: 2000..4000\n",
    "- Chunk 3: 4000..6000\n",
    "\n",
    "And so on.\n",
    "\n",
    "However, this approach has disadvantages:\n",
    "- Context loss: Important information might be split in the middle\n",
    "- Incomplete sentences: Chunks might end mid-sentence\n",
    "- Missing connections: Related information might end up in different chunks\n",
    "\n",
    "That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
    "- Chunk 1: 0..2000\n",
    "- Chunk 2: 1000..3000\n",
    "- Chunk 3: 2000..4000\n",
    "- ...\n",
    "\n",
    "This is better for AI because:\n",
    "- Continuity: Important information isn't lost at chunk boundaries\n",
    "- Context preservation: Related sentences stay together in at least one chunk\n",
    "- Better search: Queries can match information even if it spans chunk boundaries\n",
    "\n",
    "This approach is known as the \"sliding window\" method. This is how we implement it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf6a7329-6899-45b0-89da-e80d75950cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77f1c9-d465-49f1-b40b-97311d395e07",
   "metadata": {},
   "source": [
    "Let's apply it for document 45. This gives us 21 chunks:\n",
    "- 0..2000\n",
    "- 1000..3000\n",
    "- ...\n",
    "- 19000..21000\n",
    "- 20000..21712\n",
    "\n",
    "Let's process all the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a0e1d0-faea-442f-a5f7-fae4e3ec7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003a28c-18ee-44f6-9449-9bbce4ced705",
   "metadata": {},
   "source": [
    "Note that we use copy() and pop() operations:\n",
    "- doc.copy() creates a shallow copy of the document dictionary\n",
    "- doc_copy.pop('content') removes the 'content' key and returns its value\n",
    "- This way we preserve the original dictionary keys that we can use later in the chunks.\n",
    "\n",
    "This way, we obtain 575 chunks from 95 documents.\n",
    "\n",
    "We can play with the parameters by including more or less content. 2000 characters is usually good enough for RAG applications.\n",
    "\n",
    "There are some alternative approaches:\n",
    "- Token-based chunking: You first tokenize the content (turn it into a sequence of words) and then do a sliding window over tokens\n",
    "    - Advantages: More precise control over LLM input size\n",
    "    - Disadvantages: Doesn't work well for documents with code\n",
    "- Paragraph splitting: Split by paragraphs\n",
    "- Section splitting: Split by sections\n",
    "- AI-powered splitting: Let AI split the text intelligently\n",
    "\n",
    "We won't cover token-based chunking here, as we're working with documents that contain code. But it's easy to implement - ask ChatGPT for help if you need it for text-only content.\n",
    "\n",
    "We will implement the others.\n",
    "## 2. Splitting by Paragraphs and Sections\n",
    "Splitting by paragraphs is relatively easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62761d38-c309-490d-a55d-bb1c05acf7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94452d2e-f60f-40fc-8e52-30f13c45c1b0",
   "metadata": {},
   "source": [
    "We use \\n\\s*\\n regex pattern for splitting:\n",
    "- \\n matches a newline\n",
    "- \\s* matches zero or more whitespace characters\n",
    "- \\n matches another newline\n",
    "- So \\n\\s*\\n matches two newlines with optional whitespace between them\n",
    "\n",
    "This works well for literature, but it doesn't work well for documents. Most paragraphs in technical documentation are very short.\n",
    "\n",
    "You can combine sliding window and paragraph splitting for more intelligent processing. We won't do it here, but it's a good exercise to try.\n",
    "\n",
    "Let's now look at section splitting. Here, we take advantage of the documents' structure. Markdown documents have this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0f8a34b-c90d-4376-915c-1bf17fa9c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f04e0-37b8-4906-8629-ab87f7a90580",
   "metadata": {},
   "source": [
    "What we can do is split by headers.\n",
    "\n",
    "For that we will use regex too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa34bd91-71b2-4bbe-9ac1-6430e7baf1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d783d8e-baaf-4e03-aba4-ab3b4559c822",
   "metadata": {},
   "source": [
    "Note: This code may not work perfectly if we want to split by level 1 headings and have Python code with # comments. But in general, this is not a big problem for documentation.\n",
    "\n",
    "If we want to split by second-level headers, that's what we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6106775-fbcb-440d-bf3f-88cff56a8d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = split_markdown_by_level(text, level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7f25c-6ec5-4a1c-902a-66e1bf6c63e5",
   "metadata": {},
   "source": [
    "Now we iterate over all the docs to create the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dbb2fbf-ed5b-4d43-80cd-06f9b45dd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a65ec-0cdd-4b1e-a5fb-b5f5ecfdc536",
   "metadata": {},
   "source": [
    "Like previously, copy() creates a copy of the document metadata. pop('content') removes and returns the content. This way, each section gets the same metadata (title, description) as the original document.\n",
    "\n",
    "This was more intelligent processing, but we can go even further and use LLMs for that.\n",
    "## 3. Intelligent Chunking with LLM\n",
    "In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "\n",
    "This makes sense when:\n",
    "- Complex structure: Documents have complex, non-standard structure\n",
    "- Semantic coherence: You want chunks that are semantically meaningful\n",
    "- Custom logic: You need domain-specific splitting rules\n",
    "- Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "Simple approaches are sufficient. Use intelligent chunking only when\n",
    "- You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "- You have complex, unstructured documents\n",
    "- Quality is more important than cost\n",
    "- You have the budget for LLM processing\n",
    "\n",
    "Note: You can use any alternative LLM provider. One option is Groq, which is free with rate limits. You can replace the OpenAI library with the Groq library and it should work.\n",
    "\n",
    "To continue, you need to get the API key from https://platform.openai.com/api-keys (assuming you have an account).\n",
    "\n",
    "Let's stop Jupyter and create an environment variable with your key:\n",
    "```\n",
    "export OPENAI_API_KEY='your-api-key'\n",
    "export GROQ_API_KEY='your-api-key'\n",
    "```\n",
    "Install the OpenAI/Groq SDK:\n",
    "```\n",
    "uv add openai\n",
    "uv add groq\n",
    "```\n",
    "Then run jupyter notebook:\n",
    "```\n",
    "uv run jupyter notebook\n",
    "```\n",
    "It's cumbersome to set environment variables every time. I recommend using direnv, which works for Linux, Mac and Windows.\n",
    "\n",
    "Note: if you use direnv, don't forget to add .envrc to .gitignore.\n",
    "\n",
    "Warning: Never commit your API keys to git! Others can use your API key and you'll pay for it.\n",
    "\n",
    "Now we're ready to use OpenAI/Groq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b32701f4-3c35-4b27-9daf-3bd96f49d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n",
    "'''\n",
    "\n",
    "from groq import Groq\n",
    "client = Groq()\n",
    "\n",
    "def llm(prompt, model=\"llama-3.3-70b-versatile\"):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28395621-9d3a-4ac0-81c8-95dedb24b4b6",
   "metadata": {},
   "source": [
    "This code invokes an LLM (gpt-4o-mini) with the provided prompt and returns the results. We will explain in more detail what this code does in the next lessons.\n",
    "\n",
    "Let's create a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21901683-25d6-4e2c-a5d6-08884fbcfbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8104b65-0e8b-4bc5-8846-b6359597e765",
   "metadata": {},
   "source": [
    "The prompt asks the LLM to:\n",
    "- Split the document logically (not just by length)\n",
    "- Make sections self-contained\n",
    "- Use a specific output format that's easy to parse\n",
    "\n",
    "Let's create a function for intelligent chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b575679b-c906-48d1-b0cd-2d2de18a55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e44a9-91bc-41fd-8038-891665b5886e",
   "metadata": {},
   "source": [
    "Now we apply this to every document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ceeeee55-7e66-4a00-ba5b-28975f94f577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c45cbfae454407fb46ed99d681b572d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kc73gxf2es4rv6qb1873937p` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99704, Requested 1289. Please try again in 14m17.952s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m doc_copy = doc.copy()\n\u001b[32m      7\u001b[39m doc_content = doc_copy.pop(\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m sections = \u001b[43mintelligent_chunking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m section \u001b[38;5;129;01min\u001b[39;00m sections:\n\u001b[32m     11\u001b[39m     section_doc = doc_copy.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mintelligent_chunking\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mintelligent_chunking\u001b[39m(text):\n\u001b[32m      2\u001b[39m     prompt = prompt_template.format(document=text)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     response = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     sections = response.split(\u001b[33m'\u001b[39m\u001b[33m---\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     sections = [s.strip() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sections \u001b[38;5;28;01mif\u001b[39;00m s.strip()]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mllm\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm\u001b[39m(prompt, model=\u001b[33m\"\u001b[39m\u001b[33mllama-3.3-70b-versatile\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     resp = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\AI-Agents-Bootcamp\\day1_ingest_index_data\\course\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\AI-Agents-Bootcamp\\day1_ingest_index_data\\course\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\AI-Agents-Bootcamp\\day1_ingest_index_data\\course\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kc73gxf2es4rv6qb1873937p` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99704, Requested 1289. Please try again in 14m17.952s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968a7dc-e3b5-450d-81fb-d88fae00ae53",
   "metadata": {},
   "source": [
    "tqdm is a library that shows progress bars. It helps you track progress when processing a large number of documents.\n",
    "\n",
    "**Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary. For most applications, you don't need intelligent chunking.**\n",
    "\n",
    "Bonus: you can use this approach for processing the code in your GitHub repository. You can use a variation of the following prompt:\n",
    "\n",
    "\"Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\". Then add both the source code and the summary to your documents.\n",
    "## 4. How to Choose a Chunking Approach\n",
    "You may wonder - which chunking should I use? The answer: start with the simplest one and gradually increase complexity. Start with simple chunking with overlaps. We will later talk about evaluations. You can use evaluations to make informed decisions about chunking strategies.\n",
    "## Coming Up Tomorrow (Day 3)\n",
    "Our data is ready. Now we can index it – insert it into a search engine and make it available for our (future) agent to use.\n",
    "\n",
    "If you have suggestions about the course content or want to improve something, let me know! Answer to the email with this lesson.\n",
    "## Homework\n",
    "- For the project you selected, apply chunking\n",
    "- Experiment with simple chunking, paragraph chunking + sliding window, and section chunking\n",
    "- Which approach makes sense for your application? Manually inspect the results and analyze what works best\n",
    "- Make a post on social media about what you're building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68a73e-828b-4bba-8528-67cf725ab7b9",
   "metadata": {},
   "source": [
    "# Day 3: Add Search\n",
    "Welcome to Day 3 of our 7-Day AI Agents Email Crash-Course.\n",
    "\n",
    "In the first part of the course, we focus on data preparation. Before we can use data for AI agents, we need to prepare it properly.\n",
    "\n",
    "We have already downloaded the data from a GitHub repository. Yesterday (Day 2), we processed it by chunking it where necessary.\n",
    "\n",
    "Now it's time to use this data. We will index this data by putting it inside a search engine. This allows us to quickly find relevant information when users ask questions.\n",
    "\n",
    "In particular, we will:\n",
    "- Build a lexical search for exact matches and keywords\n",
    "- Implement semantic search using embeddings\n",
    "- Combine them with a hybrid search\n",
    "\n",
    "At the end of this lesson, you'll have a working search system you can query about your project. This search engine can be used later by the AI agent to look up user questions in the database.\n",
    "## 1. Text search\n",
    "The simplest type of search is a text search. Suppose we build a Q&A system for courses (using the FAQ dataset). We want to find the answer to this question:\n",
    "\n",
    "\"What should be in a test dataset for AI evaluation?\"\n",
    "\n",
    "Text search works by finding all documents that contain at least one word from the query. The more words from the query that appear in a document, the more relevant that document is.\n",
    "\n",
    "This is how modern search systems like Apache Solr or Elasticsearch work. They use indexes to efficiently search through millions of documents without having to scan each one individually.\n",
    "\n",
    "In this lesson, we'll start with a simple in-memory text search. The engine we will use is called *minsearch*.\n",
    "\n",
    "Note: This search engine was implemented as part of a workshop I held some time ago. You can find details here if you want to know how it works.\n",
    "\n",
    "Let's install it:\n",
    "```\n",
    "uv add minsearch\n",
    "```\n",
    "We will use it for chunked Evidently docs.\n",
    "\n",
    "To remind you, this is how we prepared the docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "280b764d-1858-4f98-a740-8686b6e7b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5cef6-2914-4fb7-8069-d858c36dae5a",
   "metadata": {},
   "source": [
    "You can find *read_repo_data* in the first lesson and *sliding_window* in the second lesson.\n",
    "\n",
    "Let's now index this data with minsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8939406b-4097-4de5-8be8-af1c4fbe9413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1cedcde8d70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a0334-9ed4-4e71-bcec-11c207b0f17b",
   "metadata": {},
   "source": [
    "Here we create an index that will search through four text fields: chunk content, title, description, and filename. The *keyword_fields* parameter is for exact matches (we don't need it for now).\n",
    "\n",
    "We can now use it for search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84a73770-2977-48d3-b493-3e9a5881a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d521101-2ced-4658-8142-744288a93453",
   "metadata": {},
   "source": [
    "For DataTalksClub FAQ, it's similar, except we don't need to chunk the data. For the data engineering course, it'll look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e1fd1b6-31c6-4310-b477-6069b98c3fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1ceb7443c50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c8046-c048-406d-94c5-588d31bb6d6b",
   "metadata": {},
   "source": [
    "Here we filter for only data engineering FAQ entries. We search through the question and content (answer) text fields.\n",
    "\n",
    "The result will look like this:\n",
    "```\n",
    "[{'id': '3f1424af17',\n",
    "  'question': 'Course: Can I still join the course after the start date?',\n",
    "  'content': \"Yes, even if you don't register, you're still eligible ...\",\n",
    " },\n",
    " {'id': '068529125b',\n",
    "  'question': 'Course - Can I follow the course after it finishes?',\n",
    "  'content': 'Yes, we will keep all the materials available, so you can ...',\n",
    " }\n",
    " ...\n",
    "]\n",
    "```\n",
    "This is text search, also known as \"lexical search\". We look for exact matches between our query and the documents.\n",
    "## 2. Vector search\n",
    "Text search has limitations. Consider these two queries:\n",
    "- \"I just discovered the program, can I still enroll?\"\n",
    "- \"I just found out about the course, can I still join?\"\n",
    "\n",
    "These ask the same question but share no common words (among important ones). Text search would fail to find relevant matches.\n",
    "\n",
    "This is where embeddings help. Embeddings are numerical representations of text that capture semantic meaning. Words and phrases with similar meanings have similar embeddings, even if they use different words.\n",
    "\n",
    "Vector search uses these embeddings to identify semantically similar documents, rather than just exact word matches.\n",
    "\n",
    "For vector search, we need to turn our documents into vectors (embeddings).\n",
    "\n",
    "We will use the sentence-transformers library for this purpose.\n",
    "\n",
    "Install it:\n",
    "```\n",
    "uv add sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b43c2baa-5c19-4074-9f0c-b0272626adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c53518-3613-4007-8482-1c506d479be9",
   "metadata": {},
   "source": [
    "The *multi-qa-distilbert-cos-v1* model is trained explicitly for question-answering tasks. It creates embeddings optimized for finding answers to questions.\n",
    "\n",
    "Other popular models include:\n",
    "- *all-MiniLM-L6-v2* - General-purpose, fast, and efficient\n",
    "- *all-mpnet-base-v2* - Higher quality, slower\n",
    "\n",
    "Check Sentence Transformers documentation for more options.\n",
    "\n",
    "This is how we use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebba1ebe-879c-4c6e-95d0-d6631cd53ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcdecd2-93a8-404d-af19-484fac8504d1",
   "metadata": {},
   "source": [
    "We combine the question and answer text, then convert it to an embedding vector.\n",
    "\n",
    "Let's do the same for the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e900d13-75f9-420a-a6c7-b8ea68466e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff7197-1f33-4441-8c38-3f83eebd8f5b",
   "metadata": {},
   "source": [
    "This is how we compute similarity between the query and document vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f9dbbcd-be5d-4765-8a74-66bc75f37337",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = v_query.dot(v_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e24cf-3b9f-4712-9cf1-efd1bd48e28f",
   "metadata": {},
   "source": [
    "The dot product measures similarity between vectors.\n",
    "\n",
    "Values closer to 1 indicate higher similarity, closer to 0 means lower similarity. This works because the model creates normalized embeddings where cosine similarity equals the dot product.\n",
    "\n",
    "So we can create embeddings for all documents, then compute similarity between the query and each document to find the most similar ones.\n",
    "\n",
    "This is what *VectorSearch* from *minsearch* does. Let's use it.\n",
    "\n",
    "First, we turn our docs into embeddings. This process takes time, so we'll monitor progress with tqdm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e62685d-be4b-4efd-92b1-0094a4767d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb46ddc39a54bb88baa5f65f2f9f94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0819c-a515-4310-922f-2e9ae77b57ff",
   "metadata": {},
   "source": [
    "We combine question and answer text for each FAQ entry. We convert the list to a NumPy array for efficient similarity computations.\n",
    "\n",
    "Now let's use *VectorSearch*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6330078-384b-4c5c-bbe3-a9e277c1cd88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faq_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mminsearch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorSearch\n\u001b[32m      3\u001b[39m faq_vindex = VectorSearch()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m faq_vindex.fit(\u001b[43mfaq_embeddings\u001b[49m, de_dtc_faq)\n",
      "\u001b[31mNameError\u001b[39m: name 'faq_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6c91b-8a77-45d3-ab12-6674816b1d2c",
   "metadata": {},
   "source": [
    "This creates a vector search index using our embeddings and original documents.\n",
    "\n",
    "Let's use it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403d5c8-3bcd-43a9-99fa-a79e6b1a7e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571770b1-9cd2-4695-88c2-a202c3291462",
   "metadata": {},
   "source": [
    "We first create an embedding for our query (q), then search for similar document embeddings.\n",
    "\n",
    "You can easily do the same with the Evidently docs (but only use the *chunk* field for embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ccc888-9709-49a8-a361-7e3fd9780708",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa0e32-8a88-435a-8b53-bc28ac8bd16a",
   "metadata": {},
   "source": [
    "## 3. Hybrid search\n",
    "Text search is fast and efficient. It works well for exact matches and specific terms, and requires no model inference. However, it misses semantically similar but differently worded queries and struggles to handle synonyms effectively.\n",
    "\n",
    "Vector search captures semantic meaning and handles paraphrased questions. It works with synonyms and related concepts. But it may miss exact keyword matches.\n",
    "\n",
    "Combining both approaches gives us the best of both worlds. This is known as \"hybrid search.\"\n",
    "\n",
    "The code is quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff717c-88f2-45c5-9906-e1ed62ec1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa040a5-5606-4465-af8f-98a310383d7d",
   "metadata": {},
   "source": [
    "## 4. Putting this together\n",
    "Our search is implemented!\n",
    "\n",
    "But before we can use it in our agent, we need to organize the code. Let's put all the code into different functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be52a0e-0b9d-4352-8ed4-cdff2905660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b53c52-1242-4974-963a-21312ad6ffb8",
   "metadata": {},
   "source": [
    "## 5. Selecting the best approach\n",
    "We have seen 3 approaches: text search, vector search, and hybrid search. You may wonder, how do I select the best one? We will discuss evaluation methods later in the course.\n",
    "\n",
    "But like with chunking, you should always start with the simplest approach. For search, that's text search. It's faster, easier to debug, and works well for many use cases. Only add complexity when a simple text search isn't sufficient.\n",
    "\n",
    "But let's first build our agent! Our data is ready. Tomorrow, we will build a conversational agent that can answer questions based on the data we collected.\n",
    "If you have suggestions about the course content or want to improve something, let me know!\n",
    "\n",
    "**Homework**\n",
    "- For the project you selected, index the data\n",
    "- Experiment with text and vector search\n",
    "- Which approach makes sense for your application? Manually inspect the results and analyze what works best\n",
    "- Make a post on social media about what you're building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d521a9b-0ead-465d-a676-4113e40b1331",
   "metadata": {},
   "source": [
    "# Day 4: Agents and Tools\n",
    "Welcome to day four of our AI Agents Crash Course.\n",
    "\n",
    "In the first part of the course, we focused on data preparation. Now the data is prepared and indexed so that we can use it for AI agents\n",
    "\n",
    "So far, we have done:\n",
    "- Day 1: Downloaded the data from a GitHub repository\n",
    "- Day 2: Processed it by chunking it where necessary\n",
    "- Day 3: Indexed the data so it's searchable\n",
    "\n",
    "Note that it took us quite a lot of time. We're halfway through the course, and only now we started working on agents. Most of the time so far, we have spent on data preparation.\n",
    "\n",
    "This is not a coincidence. Data preparation is the most time-consuming and critical part of building AI agents. Without properly prepared, cleaned, and indexed data, even the most sophisticated agent will provide poor results.\n",
    "\n",
    "Now it's time to create an AI agent that will use this data through the search engine that we created yesterday.\n",
    "\n",
    "This allows us to build context-aware agents. They can provide accurate, relevant answers based on your specific domain knowledge rather than just general training data.\n",
    "\n",
    "In particular, we will:\n",
    "- Learn what makes an AI system \"agentic\" through tool use\n",
    "- Build an agent that can use the search function\n",
    "- Use Pydantic AI to make it easier to implement agents\n",
    "\n",
    "At the end of this lesson, you'll have a working AI Agent that you can answer your questions in a Jupyter notebook.\n",
    "## 1. Tools and Agents\n",
    "You can find many agent definitions online.\n",
    "\n",
    "But we will use a simple one: an *agent* is an LLM that can not only generate texts, but also invoke tools. Tools are external functions that the LLM can call in order to retrieve information, perform calculations, or take actions.\n",
    "\n",
    "In our case, the agent needs to answer our questions using the content of the GitHub repository. So, the tool (only one) is a *search(query)*.\n",
    "\n",
    "But first, let's consider a situation where we have no tools at all. This is not an agent, it's just an LLM that can generate texts. Access to tools is what makes agents \"agentic\".\n",
    "\n",
    "Let's see the difference with an example.\n",
    "\n",
    "We will try asking a question without giving the LLM access to search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aeb94d16-487a-417a-9476-f811cd931cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm happy you're interested in the course. However, I need a bit more information from you. Could you please provide more details about the course you're referring to, such as its name, topic, or platform? That way, I can give you a more accurate answer about whether you can join now or not.\n"
     ]
    }
   ],
   "source": [
    "import groq\n",
    "\n",
    "groq_client = groq.Groq()\n",
    "\n",
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=chat_messages\n",
    "    )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa520292-6ba6-4527-a978-8d01f5e55a9a",
   "metadata": {},
   "source": [
    "The response is generic. In our case, it's this:\n",
    "\n",
    "“It depends on the course you're interested in. Many courses allow late enrollment, while others might have specific deadlines. I recommend checking the course's official website or contacting the instructor or administration for more details on joining.”\n",
    "\n",
    "This answer is not really useful.\n",
    "\n",
    "But if we let it invoke the *search(query)*, the agent can give us a more useful answer.\n",
    "\n",
    "Here's how the conversation would flow with our agent using the *search* tool:\n",
    "- **User**: \"I just discovered the course, can I join now?\"\n",
    "- **Agent thinking**: I can't answer this question, so I need to search for information about course enrollment and timing.\n",
    "- **Tool call**: *search(\"course enrollment join registration deadline\")*\n",
    "- **Tool response**: (...search results...)\n",
    "- **Agent response**: \"Yes, you can still join the course even after the start date...\"\n",
    "\n",
    "We will now explore how to implement it with OpenAI.\n",
    "## 2. Function Calling with OpenAI\n",
    "Let's create an agent now. In OpenAI's terminology, we'll need to use *\"function calling\"*.\n",
    "We will begin with our FAQ example and text search. You can easily extend it to vector or hybrid search or change it to the Evidently docs.\n",
    "\n",
    "This is the function we implemented yesterday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa5fe9df-d8fc-4f49-b656-a3e2d98c6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1aafb-cd70-4bdb-9d38-bb14f3cb1c82",
   "metadata": {},
   "source": [
    "We can't just pass this function to OpenAI. First, we need to describe this function, so the LLM understands how to use it.\n",
    "\n",
    "This is done using a special description format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0914301-b6c7-4561-aa9c-ad3b09c44a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"text_search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Groq\n",
    "text_search_function = {\n",
    "    \"name\": \"text_search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d5e03-4d4a-4aa4-b75e-73af21e16d80",
   "metadata": {},
   "source": [
    "This description tells OpenAI:\n",
    "- The function is called text_search\n",
    "- It searches the FAQ database\n",
    "- It takes one required parameter: query (a string)\n",
    "- The query should be the search text to look up in the course FAQ\n",
    "\n",
    "Now we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3bbc8cf-e304-493a-aabf-128244b0ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "Use the search tool before answering.\n",
    "\n",
    "\n",
    "If you need external information, call the function `text_search`.\n",
    "When calling a function:\n",
    "- Call it ONLY once\n",
    "- Provide a clean JSON argument\n",
    "- Do NOT include extra text\n",
    "\"\"\"\n",
    "\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "# OpenAI\n",
    "# response = openai_client.responses.create(\n",
    "#     model='gpt-4o-mini',\n",
    "#     input=chat_messages,\n",
    "#     tools=[text_search_tool]\n",
    "# )\n",
    "\n",
    "# Groq\n",
    "response = groq_client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=chat_messages,\n",
    "    functions=[text_search_function],\n",
    "    function_call={\n",
    "        \"name\": \"text_search\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957da6f2-d483-4859-9546-225856395d44",
   "metadata": {},
   "source": [
    "Previously, we had a simple text response; now, the response includes function calls.\n",
    "\n",
    "Let's look at *response.output*. In my case, it contains the following:\n",
    "```\n",
    "[ResponseFunctionToolCall(arguments='{\"query\":\"join course\"}', call_id='call_O0Wbg2qnqCJqAVPjpr8JSFjg', name='text_search', type='function_call', id='fc_68d53ecc2d9881948419c14db8ef11d9099f344ef74b51a3', status='completed')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d75d11f4-feff-4de0-b046-954677d54e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', annotations=None, executed_tools=None, function_call=FunctionCall(arguments='{\"query\":\"late course registration\"}', name='text_search'), reasoning=None, tool_calls=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Groq\n",
    "call = response.choices[0].message\n",
    "\n",
    "call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b07077-af92-4f3f-bc05-2852c557e36a",
   "metadata": {},
   "source": [
    "The agent analyzed the user's question and determined that to answer it, it needs to invoke the *text_search* function with the arguments *{\"query\":\"join course\"}*.\n",
    "\n",
    "Let's invoke the function with these argument:\n",
    "```\n",
    "# OpenAI\n",
    "import json\n",
    "\n",
    "call = response.output[0]\n",
    "\n",
    "arguments = json.loads(call.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": json.dumps(result),\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f04ec0b-1ac0-4274-9471-a625a9c30aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq\n",
    "import json\n",
    "\n",
    "arguments = json.loads(call.function_call.arguments)\n",
    "result = text_search(**arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7437a432-8635-4641-9ac1-ce448a5ce418",
   "metadata": {},
   "source": [
    "Here's what's happening:\n",
    "1. The LLM decided to execute a function and let us know about it\n",
    "2. We executed the function and saved the results\n",
    "3. Now we need to pass this information back to the LLM\n",
    "\n",
    "We do it by extending the *chat_messages* list and sending the entire conversation history back to the LLM:\n",
    "```\n",
    "# OpenAI\n",
    "chat_messages.append(call)\n",
    "chat_messages.append(call_output)\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a57c3ae-a811-48eb-a4d7-324e943bf4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can join the course now and start following the materials at your own pace. The course materials will be kept available even after the course finishes, so you can continue reviewing the homework and prepare for the next cohort. You can also start working on your final capstone project.\n"
     ]
    }
   ],
   "source": [
    "# Groq\n",
    "chat_messages.append(call)\n",
    "\n",
    "chat_messages.append({\n",
    "    \"role\": \"function\",\n",
    "    \"name\": \"text_search\",\n",
    "    \"content\": json.dumps(result)\n",
    "})\n",
    "\n",
    "final_response = groq_client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=chat_messages\n",
    ")\n",
    "\n",
    "print(final_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e9011-7265-4fa9-8e16-ffc9d1a07da3",
   "metadata": {},
   "source": [
    "LLMs are stateless. When we make one call to the OpenAI API and then shortly afterwards make another, it doesn't know anything about the first call. So if we only send it *call_output*, it would have no idea how to respond to it.\n",
    "\n",
    "This is why we need to send it the entire conversation history. It needs to know everything that happened so far:\n",
    "- The system prompt (so it knows what the initial instructions are) - *system_prompt*\n",
    "- The user prompt (so it knows what task it needs to perform) - *question*\n",
    "- The decision to invoke the *text_search* tool (so it knows what function was called) - that's our *call*\n",
    "- The output of the function (so it knows what the function returned) - that's our *call_output*\n",
    "\n",
    "After we invoke it, we get back the response:\n",
    "\n",
    "“Yes, you can still join the course even after the start date. While you won't be able to officially register, you are eligible to submit your homework. Just keep in mind that there are deadlines for submitting assignments and final projects, so it's best not to leave everything to the last minute.”\n",
    "\n",
    "This is a useful response that we were hoping to get.\n",
    "## 3. System Prompt: Instructions\n",
    "Let's take another look at the code we wrote previously:\n",
    "```\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "```\n",
    "We have two things here:\n",
    "- *system_prompt* contains instructions for the LLM\n",
    "- *question* (\"user prompt\") is the actual question or task\n",
    "\n",
    "The system prompt is very important: it influences how the agent behaves. This is how we can control what the agent does and how it responds to user questions.\n",
    "\n",
    "Usually, the more complete the instructions in the system prompt are, the better the results.\n",
    "\n",
    "So we can extend it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6037cd1f-f781-4861-8fe6-980c991a6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a528e-eeab-40fd-b64c-bcbb248f4c49",
   "metadata": {},
   "source": [
    "When working with agents, the system prompt becomes one of the most essential variables we can adjust to influence our agent.\n",
    "\n",
    "For example, if we want the agent to make multiple search queries, we can modify the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a422e67d-b9e0-44c8-8a7e-f261963119b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Always search for relevant information before answering. \n",
    "If the first search doesn't give you enough information, try different search terms.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f3ec7-a670-443f-aefd-8eda71d1b393",
   "metadata": {},
   "source": [
    "## 4. Pydantic AI\n",
    "Dealing with function calls can be cumbersome. We first need to understand which function we need to invoke. Then we need to pass the results back to the LLM and perform other tasks. It's easy to make a mistake there.\n",
    "\n",
    "That's why we'll use a library to handle it. There are many agentic libraries: OpenAI Agents SDK, Langchain, Pydantic AI, and many more. For educational purposes, I also implemented an agents library. It's called *ToyAIKit*. We won't use it here, but I often use it in my lessons.\n",
    "\n",
    "Today, we will use Pydantic AI. I like its API; it's simpler than other libraries and has good documentation.\n",
    "\n",
    "Let's install it:\n",
    "```\n",
    "uv add pydantic-ai\n",
    "```\n",
    "For Pydantic AI (and for other agents libraries), we don't need to describe the function in the JSON format like we did with the plain OpenAI API. The libraries take care of it.\n",
    "\n",
    "But we do need to add docstrings and type hints to our function. I asked ChatGPT to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79fb1376-32d0-4c17-aed5-016f8fb6a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e34f9-9997-42a3-9ecd-3f9575f994b6",
   "metadata": {},
   "source": [
    "We can now define an agent with Pydantic AI and give it the *text_search* tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b488c36-2128-455a-a078-712c9c94f97d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic_ai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic_ai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIChatModel\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m groq_model = \u001b[43mOpenAIChatModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllama-3.3-70b-versatile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m agent = Agent(\n\u001b[32m     22\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mfaq_agent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     instructions=system_prompt,\n\u001b[32m     24\u001b[39m     tools=[text_search],\n\u001b[32m     25\u001b[39m     model=groq_model\n\u001b[32m     26\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\AI-Agents-Bootcamp\\.venv\\Lib\\site-packages\\pydantic_ai\\models\\openai.py:435\u001b[39m, in \u001b[36mOpenAIChatModel.__init__\u001b[39m\u001b[34m(self, model_name, provider, profile, system_prompt_role, settings)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28mself\u001b[39m._model_name = model_name\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(provider, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     provider = \u001b[43minfer_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgateway/openai\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgateway\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[38;5;28mself\u001b[39m._provider = provider\n\u001b[32m    437\u001b[39m \u001b[38;5;28mself\u001b[39m.client = provider.client\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\AI-Agents-Bootcamp\\.venv\\Lib\\site-packages\\pydantic_ai\\providers\\__init__.py:169\u001b[39m, in \u001b[36minfer_provider\u001b[39m\u001b[34m(provider)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    168\u001b[39m     provider_class = infer_provider_class(provider)\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprovider_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\AI-Agents-Bootcamp\\.venv\\Lib\\site-packages\\pydantic_ai\\providers\\openai.py:85\u001b[39m, in \u001b[36mOpenAIProvider.__init__\u001b[39m\u001b[34m(self, base_url, api_key, openai_client, http_client)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m     http_client = cached_async_http_client(provider=\u001b[33m'\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28mself\u001b[39m._client = \u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\AI-Agents-Bootcamp\\.venv\\Lib\\site-packages\\openai\\_client.py:488\u001b[39m, in \u001b[36mAsyncOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    486\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    489\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    490\u001b[39m     )\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# OpenAI\n",
    "'''\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "'''\n",
    "\n",
    "# Groq\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "\n",
    "groq_model = OpenAIChatModel(\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model=groq_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58fe48e-c12a-424d-91ea-3526ede8dad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
