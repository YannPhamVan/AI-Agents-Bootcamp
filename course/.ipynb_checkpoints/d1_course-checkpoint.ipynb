{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387e668f-ca6a-4da1-b074-4ce199d454d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c95150-5c73-4b2f-84c8-f522ab0a10e2",
   "metadata": {},
   "source": [
    "# Day 1: Ingest and Index Your Data\n",
    "Welcome to our crash course!\n",
    "\n",
    "In this course, you'll learn how to build intelligent systems that can understand and interact with your data.\n",
    "\n",
    "We'll create a conversational agent that can answer questions about any GitHub repository - think of it as your personal AI assistant for documentation and code. If you know DeepWiki, it's something similar, but tailored to your GitHub repo.\n",
    "\n",
    "For that, we need to:\n",
    "- Download and process data from the repo\n",
    "- Put it inside a search engine\n",
    "- Make the search engine available to our agent\n",
    "\n",
    "In the first half of the course, we will focus on data preparation.\n",
    "\n",
    "Today, we will do the first part: downloading the data.\n",
    "## GitHub Repo Data\n",
    "On the first day, we will learn how to download and process data from any GitHub repository. We will download the data as a zip archive, process all the text data from there, and make it available for ingesting it later into a search engine.\n",
    "\n",
    "Think of this as preparing a meal - we need to gather and prep all our ingredients (the data) before we can cook (build our AI agent).\n",
    "\n",
    "Today, we will deal with simple cases, when documents are not large.\n",
    "\n",
    "Tomorrow we will deal with more complex cases when documents are big and we also have code.\n",
    "## Environment Setup\n",
    "First, let's prepare the environment. We need Python 3.10 or higher.\n",
    "\n",
    "We will use uv as the package manager. If you don't have uv, let's install it:\n",
    "```\n",
    "pip install uv\n",
    "```\n",
    "Next, create a folder aihero with two subfolders:\n",
    "- course - here you will reproduce all the examples from this email course\n",
    "- project - here you will create your own project\n",
    "Now go to course and run:\n",
    "```\n",
    "uv init\n",
    "uv add requests python-frontmatter\n",
    "uv add --dev jupyter\n",
    "```\n",
    "This will initialize an empty-  Python project with uv and install multiple libraries:\n",
    "- requests for downloading data from GitHub\n",
    "- python-frontmatter for parsing structured metadata in markdown files\n",
    "- jupyter (in dev mode)\n",
    "- \n",
    "The reason we need jupyter in dev mode is because it's only used for development and experimentation, not in the final production code.\n",
    "\n",
    "Let's start Jupyter:\n",
    "```\n",
    "uv run jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81c6f7-008c-478a-a9ab-81e446f9f73b",
   "metadata": {},
   "source": [
    "## Understanding Frontmatter\n",
    "We will also need a library for parsing frontmatter - a popular documentation format commonly used for modern frameworks like Jekyll, Hugo, and Next.js.\n",
    "\n",
    "It looks like this:\n",
    "```\n",
    "---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "date: \"2024-01-15\"\n",
    "tags: [\"ai\", \"machine-learning\", \"tutorial\"]\n",
    "difficulty: \"beginner\"\n",
    "---\n",
    "\n",
    "# Getting Started with AI\n",
    "\n",
    "This is the main content of the document written in **Markdown**.\n",
    "\n",
    "You can include code blocks, links, and other formatting here.\n",
    "\n",
    "```\n",
    "This is the main content of the document written in **Markdown**.\n",
    "\n",
    "You can include code blocks, links, and other formatting here.\n",
    "\n",
    "This format is called \"frontmatter\". The section between the --- markers contains YAML metadata that describes the document, while everything below is regular Markdown content. This is very useful because we can extract structured information (like title, tags, difficulty level) along with the content.\n",
    "\n",
    "This is how we read it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247ecd3d-45ce-460e-bf2f-7ea4abdc257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.md', 'r', encoding='utf-8') as f:\n",
    "    post = frontmatter.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a70d2a-9186-4601-a897-004ee46712de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Started with AI\n",
      "['ai', 'machine-learning', 'tutorial']\n"
     ]
    }
   ],
   "source": [
    "# Access metadata\n",
    "print(post.metadata['title'])  # \"Getting Started with AI\"\n",
    "print(post.metadata['tags'])   # [\"ai\", \"machine-learning\", \"tutorial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c758089-6631-4e88-af0f-d06c0a82d288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Getting Started with AI\n",
      "\n",
      "This is the main content of the document written in **Markdown**.\n",
      "\n",
      "You can include code blocks, links, and other formatting here.\n"
     ]
    }
   ],
   "source": [
    "# Access content\n",
    "print(post.content)  # The markdown content without frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05d052-a96d-4247-a4ef-a8d1d23af0ae",
   "metadata": {},
   "source": [
    "We can also get all the metadata and content at the same time using the post.to_dict() method.\n",
    "## Sample Repositories\n",
    "Now that we know how to process a single markdown file, let's find a repo with multiple files that we will use as our knowledge base.\n",
    "\n",
    "We will work with multiple repositories:\n",
    "- https://github.com/DataTalksClub/faq (source for https://datatalks.club/faq/) - FAQ for DataTalks.Club courses\n",
    "- https://github.com/evidentlyai/docs/ - docs for Evidently AI library\n",
    "\n",
    "There are multiple ways you can download a GitHub repo.\n",
    "First, you can clone it using git, then we process each file and prepare it for ingestion into our search system.\n",
    "\n",
    "Alternatively, we can download the entire repository as a zip file and process all the content.\n",
    "## Working with Zip Archives\n",
    "The second option is easier and more efficient for our use case.\n",
    "We don't even need to save the zip archive - we can load it into our Python process memory and extract all the data we need from there.\n",
    "So the plan:\n",
    "- Use requests for downloading the zip archive from GitHub\n",
    "- Open the archive using built-in zipfile and io modules\n",
    "- Iterate over all .md and .mdx files in the repo\n",
    "- Collect the results into a list\n",
    "\n",
    "Let's implement it step by step.\n",
    "\n",
    "Next, we download the repository as a zip file. GitHub provides a convenient URL format for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8e78b4-1974-422d-8e8a-32bd8d6b4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104ddfd-7a58-4bd3-a132-fea6f0de3665",
   "metadata": {},
   "source": [
    "Next, we download the repository as a zip file. GitHub provides a convenient URL format for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9b065b-11ea-4745-82cf-dd853040fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985a228-31cd-4719-a6ee-dac5f71ce725",
   "metadata": {},
   "source": [
    "Let's look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202f85cf-95a5-4089-9a06-ac79594d30d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '# DataTalks.Club FAQ\\n\\nA static site generator for DataTalks.Club course FAQs with automated AI-powered FAQ maintenance.\\n\\n## Features\\n\\n- **Static Site Generation**: Converts markdown FAQs to a beautiful, searchable HTML site\\n- **Automated FAQ Management**: AI-powered bot that processes new FAQ proposals\\n- **Intelligent Triage**: Automatically determines if proposals should create new entries, update existing ones, or are duplicates\\n- **GitHub Integration**: Seamless workflow via GitHub Issues and Pull Requests\\n\\n## Project Structure\\n\\n```\\nfaq/\\n├── _questions/              # FAQ content organized by course\\n│   ├── machine-learning-zoomcamp/\\n│   │   ├── _metadata.yaml   # Course configuration\\n│   │   ├── general/         # General course questions\\n│   │   ├── module-1/        # Module-specific questions\\n│   │   └── ...\\n│   ├── data-engineering-zoomcamp/\\n│   └── ...\\n├── _layouts/                # Jinja2 HTML templates\\n│   ├── base.html\\n│   ├── course.html\\n│   └── index.html\\n├── assets/                  # CSS and static assets\\n├── faq_automation/          # FAQ automation module\\n│   ├── core.py             # Core FAQ processing functions\\n│   ├── rag_agent.py        # AI-powered decision agent\\n│   ├── actions.py          # GitHub Actions integration\\n│   └── cli.py              # Command-line interface\\n├── tests/                   # Test suite\\n├── generate_website.py      # Main site generator\\n└── Makefile                # Build commands\\n```\\n\\n## Contributing FAQ Entries\\n\\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for detailed instructions.\\n\\n\\n## Development\\n\\n### Setup\\n\\n```bash\\n# Install dependencies\\nuv sync --dev\\n```\\n\\nFor testing the FAQ automation locally, you\\'ll need to set your OpenAI API key:\\n\\n```bash\\nexport OPENAI_API_KEY=\\'your-api-key-here\\'\\n```\\n\\nOr add it to your shell configuration file (e.g., `~/.bashrc`, `~/.zshrc`).\\n\\n### Running Locally\\n\\nTo test the FAQ automation locally, create a `test_issue.txt` file:\\n\\n```bash\\ncat > test_issue.txt << \\'EOF\\'\\n### Course\\nmachine-learning-zoomcamp\\n\\n### Question\\nHow do I check my Python version?\\n\\n### Answer\\nRun `python --version` in your terminal.\\nEOF\\n```\\n\\nThen process the FAQ proposal:\\n\\n```bash\\nuv run python -m faq_automation.cli \\\\\\n  --issue-body \"$(cat test_issue.txt)\" \\\\\\n  --issue-number 42\\n```\\n\\n### Testing\\n\\n```bash\\n# Generate static website\\nmake website\\n\\n# Run all tests\\nmake test\\n\\n# Run unit tests only\\nmake test-unit\\n\\n# Run integration tests only\\nmake test-int\\n```\\n\\nSee [testing documentation](tests/README.md) for detailed information about the test suite, including how to run specific test files or methods, test coverage details, and guidelines for adding new tests.\\n\\n## Architecture\\n\\n### Site Generation Pipeline\\n\\n1. **Collection** (`collect_questions()`):\\n   - Reads all markdown files from `_questions/`\\n   - Parses YAML frontmatter\\n   - Loads course metadata for section ordering\\n\\n2. **Processing** (`process_markdown()`):\\n   - Converts markdown to HTML\\n   - Applies syntax highlighting (Pygments)\\n   - Auto-links plain text URLs\\n   - Handles image placeholders\\n\\n3. **Sorting** (`sort_sections_and_questions()`):\\n   - Orders sections per `_metadata.yaml`\\n   - Sorts questions by `sort_order` field\\n\\n4. **Rendering** (`generate_site()`):\\n   - Applies Jinja2 templates\\n   - Generates course pages and index\\n   - Copies assets to `_site/`', 'filename': 'faq-main/readme.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61037193-fcdc-464a-9ff2-c5ab976128e8",
   "metadata": {},
   "source": [
    "For processing Evidently docs we also need .mdx files (React markdown), so we can modify the code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40787433-4d1a-4004-a6a3-ca5f6a89ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/evidentlyai/docs/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "424526df-94dc-4b13-b8af-7130201c4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    if not (filename.endswith('.md') or filename.endswith('.mdx')):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49eec1f1-018f-4459-a504-eb4d28f45695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Delete Plant', 'openapi': 'DELETE /plants/{id}', 'content': '', 'filename': 'docs-main/api-reference/endpoint/delete.mdx'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b89d8-dc5a-42da-b72a-14f1caeaf8a1",
   "metadata": {},
   "source": [
    "## Complete Implementation\n",
    "Let's now put everything together into a reusable function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c8e11b4-4277-4548-b330-ec10f9e77b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a88579-1abb-4e94-86e0-a2539a6f2f79",
   "metadata": {},
   "source": [
    "We can now use this function for different repositories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a0dc95-f67b-4dfb-84e2-ba3a6ac242ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1228\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafea7a5-3aa2-417f-9219-4cafc958ea48",
   "metadata": {},
   "source": [
    "## Data Processing Considerations\n",
    "For FAQ, the data is ready to use. These are small records that we can index (put into a search engine) as is.\n",
    "\n",
    "For Evidently docs, the documents are very large. We need extra processing called \"chunking\" - breaking large documents into smaller, manageable pieces. This is important because:\n",
    "1. Search relevance: Smaller chunks are more specific and relevant to user queries\n",
    "2. Performance: AI models work better with shorter text segments\n",
    "3. Memory limits: Large documents might exceed token limits of language models\n",
    "\n",
    "We will cover chunking techniques in tomorrow's lesson.\n",
    "\n",
    "If you have any suggestions about the course content or want to improve something, let me know!\n",
    "## Homework\n",
    "- Create a new uv project in the project directory\n",
    "- Select a GitHub repo with documentation (preferably with .md files)\n",
    "- Download the data from there using the techniques we've learned\n",
    "- Make a post on social media about what you're building\n",
    "# Day 2: Chunking and Intelligent Processing for Data\n",
    "Welcome to Day 2 of our 7-Day AI Agents Email Crash-Course.\n",
    "\n",
    "In the first part of the course, we focus on data preparation – the process of properly preparing data before it can be used for AI agents.\n",
    "\n",
    "**Small and Large Documents**\n",
    "\n",
    "Yesterday (Day 1), we downloaded the data from a GitHub repository and processed it. For some use cases, like the FAQ database, this is sufficient. The questions and answers are small enough. We can put them directly into the search engine.\n",
    "\n",
    "But it's different for the Evidently documentation. These documents are quite large. Let's take a look at this one: https://github.com/evidentlyai/docs/blob/main/docs/library/descriptors.mdx.\n",
    "\n",
    "We could use it as is, but we risk overwhelming our LLMs.\n",
    "\n",
    "**Why We Need to Prepare Large Documents Before Using Them**\n",
    "\n",
    "Large documents create several problems:\n",
    "- Token limits: Most LLMs have maximum input token limits\n",
    "- Cost: Longer prompts cost more money\n",
    "- Performance: LLMs perform worse with very long contexts\n",
    "- Relevance: Not all parts of a long document are relevant to a specific question\n",
    "\n",
    "So we need to split documents into smaller subdocuments. For AI applications like RAG (which we will discuss tomorrow), this process is referred to as \"chunking.\"\n",
    "## Today’s Tasks (Day 2)\n",
    "Today, we will cover multiple ways of chunking data:\n",
    "- Simple character-based chunking\n",
    "- Paragraph and section-based chunking\n",
    "- Intelligent chunking with LLM\n",
    "\n",
    "Just so you know, for the last section, you will need an OpenAI account or an account from an alternative LLM provider such as Groq.\n",
    "## 1. Simple Chunking\n",
    "Let's start with simple chunking. This will be sufficient for most cases.\n",
    "\n",
    "We can continue with the notebook from Day 1. We already downloaded the data from Evidently docs. We put them into the evidently_docs list.\n",
    "\n",
    "This is how the document at index 45 looks like:\n",
    "```\n",
    "{'title': 'LLM regression testing',\n",
    " 'description': 'How to run regression testing for LLM outputs.',\n",
    " 'content': 'In this tutorial, you will learn...'\n",
    "}\n",
    "```\n",
    "The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. For example, for size of 2000 characters, we will have:\n",
    "- chunk 1: 0..2000\n",
    "- Chunk 2: 2000..4000\n",
    "- Chunk 3: 4000..6000\n",
    "\n",
    "And so on.\n",
    "\n",
    "However, this approach has disadvantages:\n",
    "- Context loss: Important information might be split in the middle\n",
    "- Incomplete sentences: Chunks might end mid-sentence\n",
    "- Missing connections: Related information might end up in different chunks\n",
    "\n",
    "That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
    "- Chunk 1: 0..2000\n",
    "- Chunk 2: 1000..3000\n",
    "- Chunk 3: 2000..4000\n",
    "- ...\n",
    "\n",
    "This is better for AI because:\n",
    "- Continuity: Important information isn't lost at chunk boundaries\n",
    "- Context preservation: Related sentences stay together in at least one chunk\n",
    "- Better search: Queries can match information even if it spans chunk boundaries\n",
    "\n",
    "This approach is known as the \"sliding window\" method. This is how we implement it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf6a7329-6899-45b0-89da-e80d75950cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77f1c9-d465-49f1-b40b-97311d395e07",
   "metadata": {},
   "source": [
    "Let's apply it for document 45. This gives us 21 chunks:\n",
    "- 0..2000\n",
    "- 1000..3000\n",
    "- ...\n",
    "- 19000..21000\n",
    "- 20000..21712\n",
    "\n",
    "Let's process all the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a0e1d0-faea-442f-a5f7-fae4e3ec7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003a28c-18ee-44f6-9449-9bbce4ced705",
   "metadata": {},
   "source": [
    "Note that we use copy() and pop() operations:\n",
    "- doc.copy() creates a shallow copy of the document dictionary\n",
    "- doc_copy.pop('content') removes the 'content' key and returns its value\n",
    "- This way we preserve the original dictionary keys that we can use later in the chunks.\n",
    "\n",
    "This way, we obtain 575 chunks from 95 documents.\n",
    "\n",
    "We can play with the parameters by including more or less content. 2000 characters is usually good enough for RAG applications.\n",
    "\n",
    "There are some alternative approaches:\n",
    "- Token-based chunking: You first tokenize the content (turn it into a sequence of words) and then do a sliding window over tokens\n",
    "    - Advantages: More precise control over LLM input size\n",
    "    - Disadvantages: Doesn't work well for documents with code\n",
    "- Paragraph splitting: Split by paragraphs\n",
    "- Section splitting: Split by sections\n",
    "- AI-powered splitting: Let AI split the text intelligently\n",
    "\n",
    "We won't cover token-based chunking here, as we're working with documents that contain code. But it's easy to implement - ask ChatGPT for help if you need it for text-only content.\n",
    "\n",
    "We will implement the others.\n",
    "## 2. Splitting by Paragraphs and Sections\n",
    "Splitting by paragraphs is relatively easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62761d38-c309-490d-a55d-bb1c05acf7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94452d2e-f60f-40fc-8e52-30f13c45c1b0",
   "metadata": {},
   "source": [
    "We use \\n\\s*\\n regex pattern for splitting:\n",
    "- \\n matches a newline\n",
    "- \\s* matches zero or more whitespace characters\n",
    "- \\n matches another newline\n",
    "- So \\n\\s*\\n matches two newlines with optional whitespace between them\n",
    "\n",
    "This works well for literature, but it doesn't work well for documents. Most paragraphs in technical documentation are very short.\n",
    "\n",
    "You can combine sliding window and paragraph splitting for more intelligent processing. We won't do it here, but it's a good exercise to try.\n",
    "\n",
    "Let's now look at section splitting. Here, we take advantage of the documents' structure. Markdown documents have this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0f8a34b-c90d-4376-915c-1bf17fa9c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f04e0-37b8-4906-8629-ab87f7a90580",
   "metadata": {},
   "source": [
    "What we can do is split by headers.\n",
    "\n",
    "For that we will use regex too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa34bd91-71b2-4bbe-9ac1-6430e7baf1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d783d8e-baaf-4e03-aba4-ab3b4559c822",
   "metadata": {},
   "source": [
    "Note: This code may not work perfectly if we want to split by level 1 headings and have Python code with # comments. But in general, this is not a big problem for documentation.\n",
    "\n",
    "If we want to split by second-level headers, that's what we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6106775-fbcb-440d-bf3f-88cff56a8d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = split_markdown_by_level(text, level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7f25c-6ec5-4a1c-902a-66e1bf6c63e5",
   "metadata": {},
   "source": [
    "Now we iterate over all the docs to create the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dbb2fbf-ed5b-4d43-80cd-06f9b45dd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a65ec-0cdd-4b1e-a5fb-b5f5ecfdc536",
   "metadata": {},
   "source": [
    "Like previously, copy() creates a copy of the document metadata. pop('content') removes and returns the content. This way, each section gets the same metadata (title, description) as the original document.\n",
    "\n",
    "This was more intelligent processing, but we can go even further and use LLMs for that.\n",
    "## 3. Intelligent Chunking with LLM\n",
    "In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "\n",
    "This makes sense when:\n",
    "- Complex structure: Documents have complex, non-standard structure\n",
    "- Semantic coherence: You want chunks that are semantically meaningful\n",
    "- Custom logic: You need domain-specific splitting rules\n",
    "- Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "Simple approaches are sufficient. Use intelligent chunking only when\n",
    "- You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "- You have complex, unstructured documents\n",
    "- Quality is more important than cost\n",
    "- You have the budget for LLM processing\n",
    "\n",
    "Note: You can use any alternative LLM provider. One option is Groq, which is free with rate limits. You can replace the OpenAI library with the Groq library and it should work.\n",
    "\n",
    "To continue, you need to get the API key from https://platform.openai.com/api-keys (assuming you have an account).\n",
    "\n",
    "Let's stop Jupyter and create an environment variable with your key:\n",
    "```\n",
    "export OPENAI_API_KEY='your-api-key'\n",
    "export GROQ_API_KEY='your-api-key'\n",
    "```\n",
    "Install the OpenAI/Groq SDK:\n",
    "```\n",
    "uv add openai\n",
    "uv add groq\n",
    "```\n",
    "Then run jupyter notebook:\n",
    "```\n",
    "uv run jupyter notebook\n",
    "```\n",
    "It's cumbersome to set environment variables every time. I recommend using direnv, which works for Linux, Mac and Windows.\n",
    "\n",
    "Note: if you use direnv, don't forget to add .envrc to .gitignore.\n",
    "\n",
    "Warning: Never commit your API keys to git! Others can use your API key and you'll pay for it.\n",
    "\n",
    "Now we're ready to use OpenAI/Groq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b32701f4-3c35-4b27-9daf-3bd96f49d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n",
    "'''\n",
    "\n",
    "from groq import Groq\n",
    "client = Groq()\n",
    "\n",
    "def llm(prompt, model=\"llama-3.3-70b-versatile\"):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28395621-9d3a-4ac0-81c8-95dedb24b4b6",
   "metadata": {},
   "source": [
    "This code invokes an LLM (gpt-4o-mini) with the provided prompt and returns the results. We will explain in more detail what this code does in the next lessons.\n",
    "\n",
    "Let's create a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21901683-25d6-4e2c-a5d6-08884fbcfbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8104b65-0e8b-4bc5-8846-b6359597e765",
   "metadata": {},
   "source": [
    "The prompt asks the LLM to:\n",
    "- Split the document logically (not just by length)\n",
    "- Make sections self-contained\n",
    "- Use a specific output format that's easy to parse\n",
    "\n",
    "Let's create a function for intelligent chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b575679b-c906-48d1-b0cd-2d2de18a55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e44a9-91bc-41fd-8038-891665b5886e",
   "metadata": {},
   "source": [
    "Now we apply this to every document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ceeeee55-7e66-4a00-ba5b-28975f94f577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c45cbfae454407fb46ed99d681b572d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kc73gxf2es4rv6qb1873937p` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99704, Requested 1289. Please try again in 14m17.952s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m doc_copy = doc.copy()\n\u001b[32m      7\u001b[39m doc_content = doc_copy.pop(\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m sections = \u001b[43mintelligent_chunking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m section \u001b[38;5;129;01min\u001b[39;00m sections:\n\u001b[32m     11\u001b[39m     section_doc = doc_copy.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mintelligent_chunking\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mintelligent_chunking\u001b[39m(text):\n\u001b[32m      2\u001b[39m     prompt = prompt_template.format(document=text)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     response = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     sections = response.split(\u001b[33m'\u001b[39m\u001b[33m---\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     sections = [s.strip() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sections \u001b[38;5;28;01mif\u001b[39;00m s.strip()]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mllm\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm\u001b[39m(prompt, model=\u001b[33m\"\u001b[39m\u001b[33mllama-3.3-70b-versatile\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     resp = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\AI-Agents-Bootcamp\\day1_ingest_index_data\\course\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\AI-Agents-Bootcamp\\day1_ingest_index_data\\course\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\AI-Agents-Bootcamp\\day1_ingest_index_data\\course\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kc73gxf2es4rv6qb1873937p` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99704, Requested 1289. Please try again in 14m17.952s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968a7dc-e3b5-450d-81fb-d88fae00ae53",
   "metadata": {},
   "source": [
    "tqdm is a library that shows progress bars. It helps you track progress when processing a large number of documents.\n",
    "\n",
    "**Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary. For most applications, you don't need intelligent chunking.**\n",
    "\n",
    "Bonus: you can use this approach for processing the code in your GitHub repository. You can use a variation of the following prompt:\n",
    "\n",
    "\"Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\". Then add both the source code and the summary to your documents.\n",
    "## 4. How to Choose a Chunking Approach\n",
    "You may wonder - which chunking should I use? The answer: start with the simplest one and gradually increase complexity. Start with simple chunking with overlaps. We will later talk about evaluations. You can use evaluations to make informed decisions about chunking strategies.\n",
    "## Coming Up Tomorrow (Day 3)\n",
    "Our data is ready. Now we can index it – insert it into a search engine and make it available for our (future) agent to use.\n",
    "\n",
    "If you have suggestions about the course content or want to improve something, let me know! Answer to the email with this lesson.\n",
    "## Homework\n",
    "- For the project you selected, apply chunking\n",
    "- Experiment with simple chunking, paragraph chunking + sliding window, and section chunking\n",
    "- Which approach makes sense for your application? Manually inspect the results and analyze what works best\n",
    "- Make a post on social media about what you're building"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
